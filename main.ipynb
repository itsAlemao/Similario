{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "c4e6b0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import unicodedata\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "\n",
    "MEM_BYTES = 8000 #i byte che si puo tenere in memoria prima di scrivere i dati processati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "02fbfd1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# format dei vari corpus, ognuno viene riportato al mio format standard\n",
    "\n",
    "def format_paisa(in_path: str, out_path: str):\n",
    "    \"\"\"\n",
    "    Divides the paisa corpus text into paragraphs.\n",
    "    \n",
    "    The new format separates paragraphs with a newline.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(out_path):\n",
    "        open(out_path, 'x').close()\n",
    "\n",
    "    with open(in_path, 'r', encoding='utf-8') as in_f, open(out_path, 'a', encoding=\"utf-8\") as out_f:\n",
    "        for line in in_f:\n",
    "            if \"<text\" in line:\n",
    "                chunk = \"\"\n",
    "                for line2 in in_f:\n",
    "                    if \"</text>\" in line2: # ho trovato la fine del paragrafo\n",
    "                        break\n",
    "                    else:\n",
    "                        chunk += line2\n",
    "                chunk = chunk.replace('\\n', '')    \n",
    "                out_f.write(chunk + '\\n')\n",
    "                chunk = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "9fda3b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "safe_words = [\n",
    "    \"re\", \"cd\", \"tv\", \"ia\",\n",
    "    \"afa\", \"ago\", \"ala\", \"ali\", \"alt\", \"ama\", \"ami\", \"amo\", \"ano\",\n",
    "    \"ape\", \"api\", \"app\", \"avi\", \"avo\", \"bar\", \"blu\", \"boa\", \"boe\",\n",
    "    \"bot\", \"bra\", \"bue\", \"bus\", \"cai\", \"dea\", \"dei\", \"deo\", \"dio\",\n",
    "    \"dna\", \"don\", \"due\", \"dvd\", \"eco\", \"ego\", \"emo\", \"gel\", \"gin\",\n",
    "    \"gru\", \"ira\", \"ire\", \"iri\", \"iva\", \"jet\", \"lsd\", \"nei\", \"neo\",\n",
    "    \"oca\", \"odi\", \"odo\", \"ora\", \"ore\", \"ori\", \"oro\", \"pin\", \"pro\",\n",
    "    \"rum\", \"sci\", \"sim\", \"sms\", \"sis\", \"tre\", \"ufo\", \"uni\", \"uri\", \n",
    "    \"url\", \"usa\", \"uva\", \"uve\", \"web\", \"yin\", \"zen\", \"zia\", \"zie\", \n",
    "    \"zii\", \"zio\"\n",
    "]\n",
    "\n",
    "\n",
    "def clean(chunk: str):\n",
    "    \"\"\"\n",
    "    - turns all text into lowercase\n",
    "    this function:\n",
    "    - replaces into an empty space ' ' every character that is not a letter\n",
    "    - collapses multiple whitespaces\n",
    "    - saves the chunk into file 1.txt, adding a prefix and suffix to the chunk\n",
    "    - deletes all words with 3 or less characters that are not whitelisted\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        open(\"./corpus/refining_steps/1.txt\", 'x', encoding='utf-8').close()\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "\n",
    "    # 1) Lowercase\n",
    "    cleaned = chunk.lower()\n",
    "\n",
    "    # normalize accents\n",
    "    normalized = unicodedata.normalize(\"NFD\", cleaned)\n",
    "    cleaned = \"\".join(ch for ch in normalized if unicodedata.category(ch) != \"Mn\")\n",
    "\n",
    "    # 2) Keep only letters, newlines, and periods; replace everything else with space\n",
    "    cleaned = re.sub(r\"[^a-z]\", \" \", cleaned)\n",
    "    \n",
    "    # collapse multiple spaces\n",
    "    cleaned = re.sub(r\"\\s+\", \" \", cleaned)\n",
    "        \n",
    "    #remove words below 4 characters that are not whitelisted\n",
    "    filtered_words = [\n",
    "        w for w in cleaned.split()\n",
    "        if len(w) > 3 or w in safe_words\n",
    "    ]\n",
    "    cleaned = \" \".join(filtered_words)\n",
    "    return cleaned + '\\n'\n",
    "\n",
    "def format_(in_path: str, out_path: str):\n",
    "    if not os.path.exists(out_path):\n",
    "        open(out_path, 'x').close()\n",
    "\n",
    "    with open(in_path, 'r', encoding='utf-8') as in_f, open(out_path, 'a', encoding=\"utf-8\") as out_f:\n",
    "        chunk = []\n",
    "        for line in in_f:\n",
    "            chunk.append(clean(line))\n",
    "            if (sum([len(c) for c in chunk]) > MEM_BYTES): #se supero MEM_BYTES, allora scrivo sul file \n",
    "                out_f.writelines(chunk)\n",
    "                chunk.clear()\n",
    "        out_f.writelines(chunk)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "9b60ae59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# format_paisa(\"./corpus/sample.txt\", \"./corpus/refining_steps/sample_out.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "4c740809",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy \n",
    "nlp = spacy.load(\"it_core_news_lg\")\n",
    "\n",
    "def spacy_normalizer(chunk: str):\n",
    "    \"\"\"\n",
    "    This function uses spacy library to:\n",
    "\n",
    "    - remove stop words and non-words\n",
    "    - lemmatize all words\n",
    "\n",
    "    maybe repeat the paragraph with only the nouns (to be implemented...)\n",
    "\n",
    "    returns: the formatted text\n",
    "    \"\"\"\n",
    "\n",
    "    doc = nlp(chunk)\n",
    "    tokens = [\n",
    "            token.lemma_ \n",
    "            for token in doc\n",
    "            if token.is_alpha and not token.is_stop\n",
    "        ]   \n",
    "    return (\" \".join(tokens) + '\\n')\n",
    "\n",
    "\n",
    "def format_spacy(in_path: str, out_path: str):\n",
    "    chunk = []\n",
    "    with open(in_path, 'r', encoding='utf-8') as in_f, open(out_path, 'a', encoding=\"utf-8\") as out_f:\n",
    "        for line in in_f:\n",
    "            chunk.append(spacy_normalizer(line))\n",
    "            if (sum([len(c) for c in chunk]) > MEM_BYTES): \n",
    "                out_f.writelines(chunk)\n",
    "                chunk.clear()\n",
    "        out_f.writelines(chunk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "07e25e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# format_spacy(\"./corpus/refining_steps/sample_out_ft.txt\", \"./corpus/refining_steps/sample_out_ft_to_spacy.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "620ebc10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "FT_MODEL_PATH = \"/home/alessio/models/lid.176.bin\"\n",
    "ft_model = fasttext.load_model(FT_MODEL_PATH)\n",
    "\n",
    "def format_ft(in_path: str, out_path: str):\n",
    "    chunk = []\n",
    "    with open(in_path, 'r', encoding='utf-8') as in_f, open(out_path, 'a', encoding=\"utf-8\") as out_f:\n",
    "        for line in in_f:\n",
    "            chunk.append(ft_filter(line))\n",
    "            if (sum([len(c) for c in chunk]) > MEM_BYTES): #buffero fino a superare MEM_BYTES, poi scrivo sul file \n",
    "                out_f.writelines(chunk)\n",
    "                chunk.clear()\n",
    "        out_f.writelines(chunk)\n",
    "\n",
    "def ft_filter(chunk: str):\n",
    "    words = chunk.replace('\\n', \"\").split(\" \")\n",
    "    predictions = ft_model.predict(words, k=5)\n",
    "    chunk = \"\"\n",
    "\n",
    "    #debugging\n",
    "    discarded = []\n",
    "\n",
    "    for i in range(len(predictions[0])):\n",
    "        label = predictions[0][i][0]\n",
    "        if is_ok(words[i], predictions[0][i], predictions[1][i]):\n",
    "            chunk += words[i] + \" \"\n",
    "        else:\n",
    "            #debugging\n",
    "            # chunk += \"[\" + words[i] + \"] \"\n",
    "            discarded.append((words[i], predictions[0][i], predictions[1][i]))\n",
    "    \n",
    "    discarded_formatted = '\\n'.join(str(d) for d in discarded)\n",
    "    logging.debug(f\"words discarded:\\n{discarded_formatted}\")\n",
    "    chunk = chunk[:len(chunk)-1] + '\\n'\n",
    "    return chunk\n",
    "    \n",
    "def is_ok(word: str, labels, probas):\n",
    "    it_pr = probas[labels.index(\"__label__it\")] if \"__label__it\" in labels else 0\n",
    "    en_pr = probas[labels.index(\"__label__en\")] if \"__label__en\" in labels else 0\n",
    "\n",
    "    if (\n",
    "        en_pr > 0.5 and \n",
    "        it_pr < 0.05 and\n",
    "        word.lower() not in ft_safe_words\n",
    "        ) or word in ft_unsafe_words: #se la parola é sospettata di essere inglese e con pochissima probabilitá é italiana e non é in whitelist\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "# whitelist e blacklist...\n",
    "ft_safe_words = []\n",
    "ft_unsafe_words = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "d8cddfcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# format_ft(\"./corpus/refining_steps/sample_out_spacy\", \"./corpus/refining_steps/sample_out_ft.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "860a9c27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([['__label__en', '__label__it', '__label__no', '__label__ko', '__label__de']],\n",
       " [array([0.6734811 , 0.10802358, 0.0829899 , 0.03402194, 0.02290507],\n",
       "        dtype=float32)])"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft_model.predict([\"store\"], k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "4210118d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pipeline\n",
    "\n",
    "format_paisa(\"./corpus/refining_steps/sample_out.txt\", \"./corpus/refining_steps/sample_out_f.txt\")\n",
    "format_spacy(\"./corpus/refining_steps/sample_out_f.txt\", \"./corpus/refining_steps/sample_out_sp.txt\")\n",
    "format_ft(\"./corpus/refining_steps/sample_out_sp.txt\", \"./corpus/refining_steps/sample_out_ft.txt\")\n",
    "format_(\"./corpus/refining_steps/sample_out_ft.txt\", \"./corpus/refining_steps/sample_out_final.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
